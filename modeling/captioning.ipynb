{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e649eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow: 2.5.0\n",
      "keras: 2.4.3\n"
     ]
    }
   ],
   "source": [
    "# tensorflow version\n",
    "import tensorflow\n",
    "print('tensorflow: %s' % tensorflow.__version__)\n",
    "# keras version\n",
    "import keras\n",
    "print('keras: %s' % keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baf067ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      ">1000268201_693b08cb0e.jpg\n",
      ">1001773457_577c3a7d70.jpg\n",
      ">1002674143_1b742ab4b8.jpg\n",
      ">1003163366_44323f5815.jpg\n",
      ">1007129816_e794419615.jpg\n",
      ">1007320043_627395c3d8.jpg\n",
      ">1009434119_febe49276a.jpg\n",
      ">1012212859_01547e3f17.jpg\n",
      ">1015118661_980735411b.jpg\n",
      ">1015584366_dfcec3c85a.jpg\n",
      ">101654506_8eb26cfb60.jpg\n",
      ">101669240_b2d3e7f17b.jpg\n",
      ">1016887272_03199f49c4.jpg\n",
      ">1019077836_6fc9b15408.jpg\n",
      ">1019604187_d087bf9a5f.jpg\n",
      ">1020651753_06077ec457.jpg\n",
      ">1022454332_6af2c1449a.jpg\n",
      ">1022454428_b6b660a67b.jpg\n",
      ">1022975728_75515238d8.jpg\n",
      ">102351840_323e3de834.jpg\n",
      ">1024138940_f1fefbdce1.jpg\n",
      ">102455176_5f8ead62d5.jpg\n",
      ">1026685415_0431cbf574.jpg\n",
      ">1028205764_7e8df9a2ea.jpg\n",
      ">1030985833_b0902ea560.jpg\n",
      ">103106960_e8a41d64f8.jpg\n",
      ">103195344_5d2dc613a3.jpg\n",
      ">103205630_682ca7285b.jpg\n",
      ">1032122270_ea6f0beedb.jpg\n",
      ">1032460886_4a598ed535.jpg\n",
      ">1034276567_49bb87c51c.jpg\n",
      ">104136873_5b5d41be75.jpg\n",
      ">1042020065_fb3d3ba5ba.jpg\n",
      ">1042590306_95dea0916c.jpg\n",
      ">1045521051_108ebc19be.jpg\n",
      ">1048710776_bb5b0a5c7c.jpg\n",
      ">1052358063_eae6744153.jpg\n",
      ">105342180_4d4a40b47f.jpg\n",
      ">1053804096_ad278b25f1.jpg\n",
      ">1055623002_8195a43714.jpg\n",
      ">1055753357_4fa3d8d693.jpg\n",
      ">1056249424_ef2a2e041c.jpg\n",
      ">1056338697_4f7d7ce270.jpg\n",
      ">1056359656_662cee0814.jpg\n",
      ">1056873310_49c665eb22.jpg\n",
      ">1057089366_ca83da0877.jpg\n",
      ">1057210460_09c6f4c6c1.jpg\n",
      ">1057251835_6ded4ada9c.jpg\n",
      ">106490881_5a2dd9b7bd.jpg\n",
      ">106514190_bae200f463.jpg\n",
      ">1067180831_a59dc64344.jpg\n",
      ">1067675215_7336a694d6.jpg\n",
      ">1067790824_f3cc97239b.jpg\n",
      ">1072153132_53d2bb1b60.jpg\n",
      ">107318069_e9f2ef32de.jpg\n",
      ">1075716537_62105738b4.jpg\n",
      ">107582366_d86f2d3347.jpg\n",
      ">1075867198_27ca2e7efe.jpg\n",
      ">1075881101_d55c46bece.jpg\n",
      ">1077546505_a4f6c4daa9.jpg\n",
      ">1077931201_1e0bb83105.jpg\n",
      ">1079274291_9aaf896cc1.jpg\n",
      ">10815824_2997e03d76.jpg\n",
      ">1082252566_8c79beef93.jpg\n",
      ">1082379191_ec1e53f996.jpg\n",
      ">1084040636_97d9633581.jpg\n",
      ">1084104085_3b06223afe.jpg\n",
      ">1087168168_70280d024a.jpg\n",
      ">1087539207_9f77ab3aaf.jpg\n",
      ">1088767354_2acee738cf.jpg\n",
      ">108898978_7713be88fc.jpg\n",
      ">108899015_bf36131a57.jpg\n",
      ">1089181217_ee1167f7af.jpg\n",
      ">1089755335_0bfbfd30e6.jpg\n",
      ">109202756_b97fcdc62c.jpg\n",
      ">109202801_c6381eef15.jpg\n",
      ">109260216_85b0be5378.jpg\n",
      ">109260218_fca831f933.jpg\n",
      ">1093716555_801aacef79.jpg\n",
      ">1093737381_b313cd49ff.jpg\n",
      ">1094462889_f9966dafa6.jpg\n",
      ">1095476286_87d4f8664e.jpg\n",
      ">1095580424_76f0aa8a3e.jpg\n",
      ">1095590286_c654f7e5a9.jpg\n",
      ">1095980313_3c94799968.jpg\n",
      ">1096097967_ac305887b4.jpg\n",
      ">1096165011_cc5eb16aa6.jpg\n",
      ">1096395242_fc69f0ae5a.jpg\n",
      ">109671650_f7bbc297fa.jpg\n",
      ">109738763_90541ef30d.jpg\n",
      ">109738916_236dc456ac.jpg\n",
      ">109823394_83fcb735e1.jpg\n",
      ">109823395_6fb423a90f.jpg\n",
      ">109823397_e35154645f.jpg\n",
      ">1100214449_d10861e633.jpg\n",
      ">1104133405_c04a00707f.jpg\n",
      ">1105959054_9c3a738096.jpg\n",
      ">110595925_f3395c8bd6.jpg\n",
      ">1107246521_d16a476380.jpg\n",
      ">1107471216_4336c9b328.jpg\n",
      ">1110208841_5bb6806afe.jpg\n",
      ">1112212364_0c48235fc2.jpg\n",
      ">111497985_38e9f88856.jpg\n",
      ">111537217_082a4ba060.jpg\n",
      ">111537222_07e56d5a30.jpg\n",
      ">1115565519_d976d4b1f1.jpg\n",
      ">1115679311_245eff2f4b.jpg\n",
      ">111766423_4522d36e56.jpg\n",
      ">1117972841_2b9261f95f.jpg\n",
      ">1118557877_736f339752.jpg\n",
      ">1119015538_e8e796281e.jpg\n",
      ">1119418776_58e4b93eac.jpg\n",
      ">1119463452_69d4eecd08.jpg\n",
      ">1121416483_c7902d0d49.jpg\n",
      ">112178718_87270d9b4d.jpg\n",
      ">112243673_fd68255217.jpg\n",
      ">1122944218_8eb3607403.jpg\n",
      ">1124448967_2221af8dc5.jpg\n",
      ">1129704496_4a61441f2c.jpg\n",
      ">1130017585_1a219257ac.jpg\n",
      ">1130369873_d80a1aa59c.jpg\n",
      ">1130401779_8c30182e3e.jpg\n",
      ">1131155939_b4b457b05e.jpg\n",
      ">1131340021_83f46b150a.jpg\n",
      ">1131800850_89c7ffd477.jpg\n",
      ">1131804997_177c3c0640.jpg\n",
      ">1131932671_c8d17751b3.jpg\n",
      ">1132772170_600610c5df.jpg\n",
      ">113678030_87a6a6e42e.jpg\n",
      ">1138784872_69ade3f2ab.jpg\n",
      ">114051287_dd85625a04.jpg\n",
      ">1141718391_24164bf1b1.jpg\n",
      ">1141739219_2c47195e4c.jpg\n",
      ">1142283988_6b227c5231.jpg\n",
      ">1142847777_2a0c1c2551.jpg\n",
      ">1143373711_2e90b7b799.jpg\n",
      ">1143882946_1898d2eeb9.jpg\n",
      ">1144288288_e5c9558b6a.jpg\n",
      ">1148238960_f8cacec2fc.jpg\n",
      ">1149179852_acad4d7300.jpg\n",
      ">114949897_490ca7eaec.jpg\n",
      ">1151466868_3bc4d9580b.jpg\n",
      ">1153704539_542f7aa3a5.jpg\n",
      ">1155138244_859fd6e079.jpg\n",
      ">115684808_cb01227802.jpg\n",
      ">1159574340_99ba8c3c59.jpg\n",
      ">1160034462_16b38174fe.jpg\n",
      ">1160441615_fe6b3c5277.jpg\n",
      ">1163282319_b729b24c46.jpg\n",
      ">116409198_0fe0c94f3b.jpg\n",
      ">1164131282_b30926f332.jpg\n",
      ">1164765687_7aca07bbe7.jpg\n",
      ">1167662968_e466f1e80a.jpg\n",
      ">1167669558_87a8a467d6.jpg\n",
      ">1167908324_8caab45e15.jpg\n",
      ">1169307342_e7a4685a5c.jpg\n",
      ">1174525839_7c1e6cfa86.jpg\n",
      ">1174629344_a2e1a2bdbf.jpg\n",
      ">1176580356_9810d877bf.jpg\n",
      ">1177994172_10d143cb8d.jpg\n",
      ">1178705300_c224d9a4f1.jpg\n",
      ">118187095_d422383c81.jpg\n",
      ">118309463_a532b75be9.jpg\n",
      ">1184967930_9e29ce380d.jpg\n",
      ">1187435567_18173c148b.jpg\n",
      ">1187593464_ce862352c6.jpg\n",
      ">1189977786_4f5aaed773.jpg\n",
      ">1191338263_a4fa073154.jpg\n",
      ">1193116658_c0161c35b5.jpg\n",
      ">119534510_d52b3781a3.jpg\n",
      ">1197800988_7fb0ca4888.jpg\n",
      ">1198194316_543cc7b945.jpg\n",
      ">1204996216_71d7519d9a.jpg\n",
      ">1206506157_c7956accd5.jpg\n",
      ">1207159468_425b902bfb.jpg\n",
      ">1211015912_9f3ee3a995.jpg\n",
      ">1213336750_2269b51397.jpg\n",
      ">1215334959_b1970965f7.jpg\n",
      ">121800200_bef08fae5f.jpg\n",
      ">121971540_0a986ee176.jpg\n",
      ">1220401002_3f44b1f3f7.jpg\n",
      ">1222322358_225067636e.jpg\n",
      ">1224851143_33bcdd299c.jpg\n",
      ">1225443522_1633e7121f.jpg\n",
      ">1227655020_b11a1bb112.jpg\n",
      ">1229756013_94663527d7.jpg\n",
      ">1231229740_8dcbf80bfb.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1232148178_4f45cc3284.jpg\n",
      ">1234293791_6566284bcd.jpg\n",
      ">1234817607_924893f6e1.jpg\n",
      ">1235580648_7eebaed9bc.jpg\n",
      ">1235681222_819231767a.jpg\n",
      ">1235685934_be89b231fb.jpg\n",
      ">1236951314_0308dc4138.jpg\n",
      ">1236964638_1808784a3c.jpg\n",
      ">1237985362_dbafc59280.jpg\n",
      ">123889082_d3751e0350.jpg\n",
      ">123997871_6a9ca987b1.jpg\n",
      ">1240297429_c36ae0c58f.jpg\n",
      ">124195430_d14028660f.jpg\n",
      ">1244140539_da4804d828.jpg\n",
      ">1244306891_8e78ae1620.jpg\n",
      ">1244485675_822e6efe60.jpg\n",
      ">1245022983_fb329886dd.jpg\n",
      ">1247181182_35cabd76f3.jpg\n",
      ">1248357227_2b4175fc39.jpg\n",
      ">1248734482_3038218f3b.jpg\n",
      ">124881487_36e668145d.jpg\n",
      ">1248940539_46d33ed487.jpg\n",
      ">1248953128_24c9f8d924.jpg\n",
      ">124972799_de706b6d0b.jpg\n",
      ">1251558317_4ef844b775.jpg\n",
      ">1252396628_eb81d3905b.jpg\n",
      ">1252787177_4b08625897.jpg\n",
      ">125319704_49ead3463c.jpg\n",
      ">1253264731_e7c689eca5.jpg\n",
      ">1253275679_e955fb7304.jpg\n",
      ">1255504166_f2437febcb.jpg\n",
      ">1258913059_07c613f7ff.jpg\n",
      ">1259936608_e3f0064f23.jpg\n",
      ">1260816604_570fc35836.jpg\n",
      ">1262077938_8b9516c273.jpg\n",
      ">1262454669_f1caafec2d.jpg\n",
      ">1262583859_653f1469a9.jpg\n",
      ">1263126002_881ebd7ac9.jpg\n",
      ">1263801010_5c74bf1715.jpg\n",
      ">1267711451_e2a754b4f8.jpg\n",
      ">1269470943_ba7fc49b4d.jpg\n",
      ">1271210445_7f7ecf3791.jpg\n",
      ">1271960365_e54033f883.jpg\n",
      ">1273001772_1585562051.jpg\n",
      ">127450902_533ceeddfc.jpg\n",
      ">127488876_f2d2a89588.jpg\n",
      ">127490019_7c5c08cb11.jpg\n",
      ">1277185009_06478dd457.jpg\n",
      ">1277743944_f4e8c78403.jpg\n",
      ">1280147517_98767ca3b3.jpg\n",
      ">1280320287_b2a4b9b7bd.jpg\n",
      ">1282392036_5a0328eb86.jpg\n",
      ">12830823_87d2654e31.jpg\n",
      ">1285067106_2adc307240.jpg\n",
      ">1285874746_486731a954.jpg\n",
      ">1286408831_05282582ed.jpg\n",
      ">1287064529_aa4e4f3c31.jpg\n",
      ">1287073593_f3d2a62455.jpg\n",
      ">1287475186_2dee85f1a5.jpg\n",
      ">1287920676_d21a0b289b.jpg\n",
      ">1287931016_fb015e2e10.jpg\n",
      ">1287982439_6578006e22.jpg\n",
      ">1288909046_d2b2b62607.jpg\n",
      ">128912885_8350d277a4.jpg\n",
      ">1289142574_2bd6a082dd.jpg\n",
      ">1290894194_8a4ffdc7eb.jpg\n",
      ">1294578091_2ad02fea91.jpg\n",
      ">1295669416_21cabf594d.jpg\n",
      ">1295671216_cde1b9c9d1.jpg\n",
      ">1295698260_e10c53c137.jpg\n",
      ">129599450_cab4e77343.jpg\n",
      ">1296412797_85b6d2f8d6.jpg\n",
      ">1296770308_3db2022f5a.jpg\n",
      ">1298295313_db1f4c6522.jpg\n",
      ">1298866571_b4c496b71c.jpg\n",
      ">1299459550_1fd5594fa2.jpg\n",
      ">1299459562_ed0e064aee.jpg\n",
      ">1301140633_046e4e8010.jpg\n",
      ">130211457_be3f6b335d.jpg\n",
      ">1302657647_46b36c0d66.jpg\n",
      ">1303335399_b3facd47ab.jpg\n",
      ">1303548017_47de590273.jpg\n",
      ">1303550623_cb43ac044a.jpg\n",
      ">1303727066_23d0f6ed43.jpg\n",
      ">1303727828_d1052ee341.jpg\n",
      ">1304100320_c8990a1539.jpg\n",
      ">1304961697_76b86b0c18.jpg\n",
      ">1305564994_00513f9a5b.jpg\n",
      ">1306145560_1e37081b91.jpg\n",
      ">1307635496_94442dc21a.jpg\n",
      ">1308472581_9961782889.jpg\n",
      ">1308617539_54e1a3dfbe.jpg\n",
      ">1309330801_aeeb23f1ee.jpg\n",
      ">1311132744_5ffd03f831.jpg\n",
      ">1311388430_4ab0cd1a1f.jpg\n",
      ">1312020846_5abb4a9be2.jpg\n",
      ">1312227131_771b5ed201.jpg\n",
      ">1312954382_cf6d70d63a.jpg\n",
      ">1313693129_71d0b21c63.jpg\n",
      ">1313961775_824b87d155.jpg\n",
      ">1316247213_1d2c726dd5.jpg\n",
      ">131632409_4de0d4e710.jpg\n",
      ">1317292658_ba29330a0b.jpg\n",
      ">1319634306_816f21677f.jpg\n",
      ">1321723162_9d4c78b8af.jpg\n",
      ">1321949151_77b77b4617.jpg\n",
      ">1322323208_c7ecb742c6.jpg\n",
      ">1324816249_86600a6759.jpg\n",
      ">132489044_3be606baf7.jpg\n",
      ">1329832826_432538d331.jpg\n",
      ">1330645772_24f831ff8f.jpg\n",
      ">133189853_811de6ab2a.jpg\n",
      ">1332208215_fa824f6659.jpg\n",
      ">1332492622_8c66992b62.jpg\n",
      ">1332722096_1e3de8ae70.jpg\n",
      ">1332815795_8eea44375e.jpg\n",
      ">1332823164_c70a5d930e.jpg\n",
      ">1333888922_26f15c18c3.jpg\n",
      ">1334892555_1beff092c3.jpg\n",
      ">1335617803_4fbc03dab0.jpg\n",
      ">1337792872_d01a390b33.jpg\n",
      ">1338523142_57fce8229b.jpg\n",
      ">133905560_9d012b47f3.jpg\n",
      ">1339596997_8ac29c1841.jpg\n",
      ">1341787777_4f1ebb1793.jpg\n",
      ">1342766791_1e72f92455.jpg\n",
      ">1342780478_bacc32344d.jpg\n",
      ">1343426964_cde3fb54e8.jpg\n",
      ">1346051107_9cdc14e070.jpg\n",
      ">1346529555_e916816cfe.jpg\n",
      ">134724228_30408cd77f.jpg\n",
      ">1347519824_e402241e4f.jpg\n",
      ">1348113612_5bfc5f429e.jpg\n",
      ">1348304997_afe60a61df.jpg\n",
      ">1348891916_ebd4413033.jpg\n",
      ">134894450_dadea45d65.jpg\n",
      ">1348947380_14f0fc1237.jpg\n",
      ">1348957576_c4a78eb974.jpg\n",
      ">1350811702_2ce7cfd0c5.jpg\n",
      ">1350948838_fdebe4ff65.jpg\n",
      ">1351315701_6580b51c41.jpg\n",
      ">1351764581_4d4fb1b40f.jpg\n",
      ">135235570_5698072cd4.jpg\n",
      ">1352398363_9cc8ffcce9.jpg\n",
      ">1352410176_af6b139734.jpg\n",
      ">1354318519_2f9baed754.jpg\n",
      ">1355450069_c0675b0706.jpg\n",
      ">1355703632_5683a4b6fb.jpg\n",
      ">1355833561_9c43073eda.jpg\n",
      ">1355935187_2c99648138.jpg\n",
      ">1355945307_f9e01a9a05.jpg\n",
      ">1356543628_c13ebe38fb.jpg\n",
      ">1356796100_b265479721.jpg\n",
      ">1357689954_72588dfdc4.jpg\n",
      ">1357724865_4faf4e1418.jpg\n",
      ">1357753846_6185e26040.jpg\n",
      ">1358089136_976e3d2e30.jpg\n",
      ">1358892595_7a37c45788.jpg\n",
      ">1359101233_16c2c150e3.jpg\n",
      ">1361420539_e9599c60ae.jpg\n",
      ">1362128028_8422d53dc4.jpg\n",
      ">1363843090_9425d93064.jpg\n",
      ">1363924449_487f0733df.jpg\n",
      ">136552115_6dc3e7231c.jpg\n",
      ">136639119_6040b00946.jpg\n",
      ">136644343_0e2b423829.jpg\n",
      ">136644885_f7d2bbf546.jpg\n",
      ">1368338041_6b4077ca98.jpg\n",
      ">1368383637_614646cc4a.jpg\n",
      ">136886677_6026c622eb.jpg\n",
      ">1370615506_2b96105ca3.jpg\n",
      ">1370773415_967b1ffde1.jpg\n",
      ">1377668044_36398401dd.jpg\n",
      ">1378557186_4bd1da6834.jpg\n",
      ">1379026456_153fd8b51b.jpg\n",
      ">1383698008_8ac53ed7ec.jpg\n",
      ">1383840121_c092110917.jpg\n",
      ">1384292980_4022a7520c.jpg\n",
      ">1386251841_5f384a0fea.jpg\n",
      ">1386964743_9e80d96b05.jpg\n",
      ">138705546_be7a6845dd.jpg\n",
      ">138718600_f430ebca17.jpg\n",
      ">1387443857_602ab6f9bf.jpg\n",
      ">1387461595_2fe6925f73.jpg\n",
      ">1387785218_cee67735f5.jpg\n",
      ">1388346434_524d0b6dfa.jpg\n",
      ">1388373425_3c72b56639.jpg\n",
      ">1388970365_162edcceb4.jpg\n",
      ">1389264266_8170bc1c54.jpg\n",
      ">1389323170_d1c81d6b51.jpg\n",
      ">1389651420_8d95d8f6ed.jpg\n",
      ">1390268323_2c8204e91c.jpg\n",
      ">1392272228_cf104086e6.jpg\n",
      ">1394368714_3bc7c19969.jpg\n",
      ">1394396709_65040d97ab.jpg\n",
      ">1394599090_fe0ba238f0.jpg\n",
      ">1394620454_bf708cc501.jpg\n",
      ">1394927474_0afdd82fc4.jpg\n",
      ">1396064003_3fd949c9dd.jpg\n",
      ">1396703063_e8c3687afe.jpg\n",
      ">1397295388_8a5b6b525d.jpg\n",
      ">1397887419_e798697b93.jpg\n",
      ">1397923690_d3bf1f799e.jpg\n",
      ">1398606571_f543f7698a.jpg\n",
      ">1398613231_18de248606.jpg\n",
      ">1398873613_7e3174dd6c.jpg\n",
      ">1400424834_1c76e700c4.jpg\n",
      ">1401961581_76921a75c5.jpg\n",
      ">1402640441_81978e32a9.jpg\n",
      ">1402641725_5e027ecaa7.jpg\n",
      ">1402843760_d30f1dbf0f.jpg\n",
      ">1402859872_0fc8cf8108.jpg\n",
      ">1403414927_5f80281505.jpg\n",
      ">140377584_12bdbdf2f8.jpg\n",
      ">140430106_2978fda105.jpg\n",
      ">1404832008_68e432665b.jpg\n",
      ">1405221276_21634dcd58.jpg\n",
      ">140526326_da07305c1c.jpg\n",
      ">140526327_3cb984de09.jpg\n",
      ">1406010299_5755339f08.jpg\n",
      ">1408958345_68eea9a4e4.jpg\n",
      ">1410193619_13fff6c875.jpg\n",
      ">141139674_246c0f90a1.jpg\n",
      ">141140165_9002a04f19.jpg\n",
      ">1412832223_99e8b4701a.jpg\n",
      ">1413956047_c826f90c8b.jpg\n",
      ">1414779054_31946f9dfc.jpg\n",
      ">1414820925_3504c394e1.jpg\n",
      ">1415591512_a84644750c.jpg\n",
      ">1417031097_ab656bc4bd.jpg\n",
      ">1417295167_5299df6db8.jpg\n",
      ">141755290_4b954529f3.jpg\n",
      ">141755292_7a0b3364cf.jpg\n",
      ">1417637704_572b4d6557.jpg\n",
      ">1417882092_c94c251eb3.jpg\n",
      ">1417941060_2a0f7908bc.jpg\n",
      ">1418019748_51c7d59c11.jpg\n",
      ">1418266617_b32143275b.jpg\n",
      ">1418503947_953d373632.jpg\n",
      ">1419286010_b59af3962a.jpg\n",
      ">1419385780_1383ec7ba9.jpg\n",
      ">1420060020_7a6984e2ea.jpg\n",
      ">1420060118_aed262d606.jpg\n",
      ">1423126855_6cd2a3956c.jpg\n",
      ">1423997242_ea2189ec5e.jpg\n",
      ">1424237335_b3be9920ba.jpg\n",
      ">1424775129_ffea9c13ab.jpg\n",
      ">1425013325_bff69bc9da.jpg\n",
      ">1425069308_488e5fcf9d.jpg\n",
      ">1425069590_570cc7c2d8.jpg\n",
      ">1425485485_d7c97a5470.jpg\n",
      ">1425919702_ddb761aeec.jpg\n",
      ">1426014905_da60d72957.jpg\n",
      ">1427391496_ea512cbe7f.jpg\n",
      ">142802798_962a4ec5ce.jpg\n",
      ">1428578577_82864facae.jpg\n",
      ">1428641354_f7453afbea.jpg\n",
      ">1428681303_04213524e3.jpg\n",
      ">1429546659_44cb09cbe2.jpg\n",
      ">1429723917_6af585e4c0.jpg\n",
      ">1429814475_0b592b9995.jpg\n",
      ">1430154945_71bbaa094a.jpg\n",
      ">1432179046_8e3d75cf81.jpg\n",
      ">1432342377_3e41603f26.jpg\n",
      ">143237785_93f81b3201.jpg\n",
      ">1433088025_bce2cb69f8.jpg\n",
      ">1433142189_cda8652603.jpg\n",
      ">1433397131_8634fa6664.jpg\n",
      ">1433577867_39a1510c43.jpg\n",
      ">1434005938_ad75c8598c.jpg\n",
      ">1434607942_da5432c28c.jpg\n",
      ">143552697_af27e9acf5.jpg\n",
      ">143552829_72b6ba49d4.jpg\n",
      ">1436760519_8d6101a0ed.jpg\n",
      ">143680442_2f03f76944.jpg\n",
      ">143680966_0010ff8c60.jpg\n",
      ">143684568_3c59299bae.jpg\n",
      ">143688205_630813a466.jpg\n",
      ">143688283_a96ded20f1.jpg\n",
      ">143688895_e837c3bc76.jpg\n",
      ">1439046601_cf110a75a7.jpg\n",
      ">1439282131_3814d6ae04.jpg\n",
      ">1440024115_129212c988.jpg\n",
      ">1443807993_aebfb2784a.jpg\n",
      ">1445123245_c7b9db0e0c.jpg\n",
      ">1445754124_647168f211.jpg\n",
      ">1446053356_a924b4893f.jpg\n",
      ">1446933195_8fe9725d62.jpg\n",
      ">1448511770_1a4a9c453b.jpg\n",
      ">1449370354_380c4123c9.jpg\n",
      ">1449625950_fc9a8d02d9.jpg\n",
      ">1449692616_60507875fb.jpg\n",
      ">1452361926_6d8c535e32.jpg\n",
      ">1453366750_6e8cf601bf.jpg\n",
      ">1454678644_7e5a371301.jpg\n",
      ">1454841725_4b6e6199e2.jpg\n",
      ">1456393634_74022d9056.jpg\n",
      ">1456630952_dd4778a48f.jpg\n",
      ">145721496_687af9bb18.jpg\n",
      ">145721498_a27d2db576.jpg\n",
      ">1457762320_7fe121b285.jpg\n",
      ">1459032057_97e73ed6ab.jpg\n",
      ">1459250022_bf1eddad11.jpg\n",
      ">1460352062_d64fb633e0.jpg\n",
      ">1460500597_866fa0c6f3.jpg\n",
      ">146098876_0d99d7fb98.jpg\n",
      ">146100443_906d87faa2.jpg\n",
      ">1461329041_c623b06e5b.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1461653394_8ab96aae63.jpg\n",
      ">1461667284_041c8a2475.jpg\n",
      ">1463638541_c02cfa04dc.jpg\n",
      ">1463732130_a754441289.jpg\n",
      ">1463732807_0cdf4f22c7.jpg\n",
      ">1464120327_d90279ca3a.jpg\n",
      ">1465666502_de289b3b9c.jpg\n",
      ">146577645_91b570c0d0.jpg\n",
      ">146577646_9e64b8c2dc.jpg\n",
      ">1466307485_5e6743332e.jpg\n",
      ">1466307489_cb8a74de09.jpg\n",
      ">1466479163_439db855af.jpg\n",
      ">1467533293_a2656cc000.jpg\n",
      ">1468103286_96a6e07029.jpg\n",
      ">1468389504_c724bdcad0.jpg\n",
      ">1468429623_f001988691.jpg\n",
      ">1468962616_5803b4397f.jpg\n",
      ">1469000260_5d473c8283.jpg\n",
      ">1469358746_2a879abaf3.jpg\n",
      ">1470061031_4cb59c12a8.jpg\n",
      ">1470132731_fa416b7504.jpg\n",
      ">1470536919_1f3fd6c65a.jpg\n",
      ">1472053993_bed67a3ba7.jpg\n",
      ">1472230829_803818a383.jpg\n",
      ">1472249944_d887c3aeda.jpg\n",
      ">1472653060_7427d2865a.jpg\n",
      ">1472882567_33dc14c8b6.jpg\n",
      ">1473080948_bae2925dc8.jpg\n",
      ">1473250020_dc829a090f.jpg\n",
      ">1473618073_7db56a5237.jpg\n",
      ">1474474514_b3eb492722.jpg\n",
      ">1475046848_831245fc64.jpg\n",
      ">1476002408_4256b7b2fa.jpg\n",
      ">1476241331_2f43b67aed.jpg\n",
      ">1478268555_7e301fc510.jpg\n",
      ">1478294229_7e1c822fea.jpg\n",
      ">1478606153_a7163bf899.jpg\n",
      ">1479028910_3dab3448c8.jpg\n",
      ">1479124077_17dcc0d5d7.jpg\n",
      ">1479513774_70c94cf9d3.jpg\n",
      ">1479679558_d0a01bc62b.jpg\n",
      ">1479857177_9d4a6f38fd.jpg\n",
      ">1480712062_32a61ad4b7.jpg\n",
      ">1481062342_d9e34366c4.jpg\n",
      ">1482960952_95f2d419cb.jpg\n",
      ">148512773_bae6901fd6.jpg\n",
      ">1488937076_5baa73fc2a.jpg\n",
      ">1489286545_8df476fa26.jpg\n",
      ">1490213660_9ea45550cf.jpg\n",
      ">1490670858_e122df2560.jpg\n",
      ">1491192153_7c395991e5.jpg\n",
      ">1499495021_d295ce577c.jpg\n",
      ">1499554025_a8ffe0e479.jpg\n",
      ">1499581619_a5f65a882c.jpg\n",
      ">1500853305_0150615ce9.jpg\n",
      ">1501297480_8db52c15b0.jpg\n",
      ">1501811302_5e723fc529.jpg\n",
      ">1501985304_8c50093004.jpg\n",
      ">150387174_24825cf871.jpg\n",
      ">1505686764_9e3bcd854a.jpg\n",
      ">150582765_bad8dec237.jpg\n",
      ">1507563902_6ec8d5d822.jpg\n",
      ">1508269285_6c5723f67d.jpg\n",
      ">1509786421_f03158adfc.jpg\n",
      ">1510078253_96e9ec50e7.jpg\n",
      ">1510669311_75330b4781.jpg\n",
      ">1511807116_41c3645e8c.jpg\n",
      ">1514957266_a19827c538.jpg\n",
      ">1515025681_999199cb79.jpg\n",
      ">1515883224_14e36a53c7.jpg\n",
      ">1516714577_7d1c35a8d8.jpg\n",
      ">1517340899_ee1c74a8f6.jpg\n",
      ">1517721825_10176d0683.jpg\n",
      ">1517807181_ca6588f2a0.jpg\n",
      ">152029243_b3582c36fa.jpg\n",
      ">1521623639_4bda3407cc.jpg\n",
      ">1522787272_5a31497ef2.jpg\n",
      ">1523800748_a59e980eee.jpg\n",
      ">1523984678_edd68464da.jpg\n",
      ">1525153022_06c48dbe52.jpg\n",
      ">1526181215_c1a94325ae.jpg\n",
      ">1526325728_74eb4153d8.jpg\n",
      ">1527297882_dededc7891.jpg\n",
      ">1527333441_af65636a74.jpg\n",
      ">1527513023_3d8152b379.jpg\n",
      ">1528205014_1323aa9dfd.jpg\n",
      ">1529044279_4922ead27c.jpg\n",
      ">1536597926_c2e1bc2379.jpg\n",
      ">1536774449_e16b1b6382.jpg\n",
      ">1539166395_0cdc0accee.jpg\n",
      ">1540631615_8b42c1b160.jpg\n",
      ">1541272333_1624b22546.jpg\n",
      ">1542033433_5453d4c466.jpg\n",
      ">1547883892_e29b3db42e.jpg\n",
      ">154871781_ae77696b77.jpg\n",
      ">1550772959_9ca9fa625f.jpg\n",
      ">1552065993_b4dcd2eadf.jpg\n",
      ">155221027_b23a4331b7.jpg\n",
      ">1554713437_61b64527dd.jpg\n",
      ">1557451043_f5c91ff6f4.jpg\n",
      ">1557838421_a33f2a4911.jpg\n",
      ">1561658940_a947f2446a.jpg\n",
      ">1562392511_522a26063b.jpg\n",
      ">1562478333_43d13e5427.jpg\n",
      ">1562478713_505ab6d924.jpg\n",
      ">1563731247_7f21d8bec0.jpg\n",
      ">1564614124_0ee6799935.jpg\n",
      ">1566117559_f5d98fbeb0.jpg\n",
      ">1569562856_eedb5a0a1f.jpg\n",
      ">156967462_72db9b722c.jpg\n",
      ">1569687608_0e3b3ad044.jpg\n",
      ">1570723692_3a2b064d43.jpg\n",
      ">157139628_5dc483e2e4.jpg\n",
      ">1572286502_64e5c4b920.jpg\n",
      ">1572532018_64c030c974.jpg\n",
      ">1573017288_4d481856e2.jpg\n",
      ">1574401950_6bedc0d29b.jpg\n",
      ">1576185717_f841ddc3da.jpg\n",
      ">1579198375_84b18e003a.jpg\n",
      ">1579206585_5ca6a24db0.jpg\n",
      ">1579287915_4257c54451.jpg\n",
      ">1579798212_d30844b4c5.jpg\n",
      ">1580172290_e19067e0dd.jpg\n",
      ">1580671272_3e99d94305.jpg\n",
      ">1581822598_0ae23074f1.jpg\n",
      ">1584315962_5b0b45d02d.jpg\n",
      ">1594038143_57f299aa8a.jpg\n",
      ">159712188_d530dd478c.jpg\n",
      ">1597319381_1e80d9e39c.jpg\n",
      ">1597557856_30640e0b43.jpg\n",
      ">1598085252_f3219b6140.jpg\n",
      ">1600208439_e94527b80f.jpg\n",
      ">160541986_d5be2ab4c1.jpg\n",
      ">160566014_59528ff897.jpg\n",
      ">160585932_fa6339f248.jpg\n",
      ">1606988704_fe330878a3.jpg\n",
      ">160792599_6a7ec52516.jpg\n",
      ">160805827_5e6646b753.jpg\n",
      ">1616016569_673de1d678.jpg\n",
      ">161669933_3e7d8c7e2c.jpg\n",
      ">161905204_247c6ca6de.jpg\n",
      ">1620397000_3883e3ecd3.jpg\n",
      ">162152393_52ecd33fc5.jpg\n",
      ">1622619190_d0b51aff28.jpg\n",
      ">1625306051_7099519baa.jpg\n",
      ">1626754053_81126b67b6.jpg\n",
      ">162743064_bb242faa31.jpg\n",
      ">1643915227_9f48068772.jpg\n",
      ">1650280501_29810b46e5.jpg\n",
      ">1655781989_b15ab4cbff.jpg\n",
      ">1659358133_95cd1027bd.jpg\n",
      ">1659358141_0433c9bf99.jpg\n",
      ">1659396176_ced00a549f.jpg\n",
      ">1662261486_db967930de.jpg\n",
      ">166321294_4a5e68535f.jpg\n",
      ">1663454406_5e2cf8c5bb.jpg\n",
      ">1663751778_90501966f0.jpg\n",
      ">166433861_70b66cd381.jpg\n",
      ">166507476_9be5b9852a.jpg\n",
      ">166654939_80ea4ddbcc.jpg\n",
      ">1670592963_39731a3dac.jpg\n",
      ">167295035_336f5f5f27.jpg\n",
      ">1674612291_7154c5ab61.jpg\n",
      ">1675679141_36c9bc2969.jpg\n",
      ">1676601498_7d59327523.jpg\n",
      ">1679557684_50a206e4a9.jpg\n",
      ">1679565118_d36f0d6d52.jpg\n",
      ">1679617928_a73c1769be.jpg\n",
      ">1680126311_b92a2e8e72.jpg\n",
      ">1682079482_9a72fa57fa.jpg\n",
      ">1683444418_815f660379.jpg\n",
      ">1685463722_55843b6d3c.jpg\n",
      ">1685990174_09c4fb7df8.jpg\n",
      ">1688699579_2f72328c7e.jpg\n",
      ">1689658980_0074d81d28.jpg\n",
      ">1691573772_1adef8e40e.jpg\n",
      ">169490297_b6ff13632a.jpg\n",
      ">170100272_d820db2199.jpg\n",
      ">1713248047_d03721456d.jpg\n",
      ">1713248099_d860df4e10.jpg\n",
      ">1714316707_8bbaa2a2ba.jpg\n",
      ">171488318_fb26af58e2.jpg\n",
      ">1716445442_9cf3528342.jpg\n",
      ">1718184338_5968d88edb.jpg\n",
      ">172092461_a9a9762e13.jpg\n",
      ">172092464_d9eb4f4f2f.jpg\n",
      ">172097782_f0844ec317.jpg\n",
      ">172097783_292c5413d8.jpg\n",
      ">1721637099_93e9ec2a2f.jpg\n",
      ">17273391_55cfc7d3d4.jpg\n",
      ">173020287_230bfc4ffc.jpg\n",
      ">1731546544_9fbf14617b.jpg\n",
      ">1732217138_aa0199ef87.jpg\n",
      ">1732436777_950bcdc9b8.jpg\n",
      ">174466741_329a52b2fe.jpg\n",
      ">1745110280_0cbff5e273.jpg\n",
      ">1752454466_723790dbd6.jpg\n",
      ">1763020597_d4cc8f0f8a.jpg\n",
      ">1764955991_5e53a28c87.jpg\n",
      ">1765164972_92dac06fa9.jpg\n",
      ">1770036088_08abe4f6e9.jpg\n",
      ">1771490732_0ab5f029ac.jpg\n",
      ">1772859261_236c09b861.jpg\n",
      ">177302997_5b2d770a0a.jpg\n",
      ">1773928579_5664a810dc.jpg\n",
      ">1775029934_e1e96038a8.jpg\n",
      ">1776981714_5b224d0f7a.jpg\n",
      ">1777816180_08d7e8063b.jpg\n",
      ">1778020185_1d44c04dae.jpg\n",
      ">1781227288_6811e734be.jpg\n",
      ">1784309115_0ad6791146.jpg\n",
      ">1785138090_76a56aaabc.jpg\n",
      ">1786425974_c7c5ad6aa1.jpg\n",
      ">1787222774_d5c68cce53.jpg\n",
      ">179009558_69be522c63.jpg\n",
      ">1794818900_e0ffdd268e.jpg\n",
      ">1795151944_d69b82f942.jpg\n",
      ">1797507760_384744fb34.jpg\n",
      ">1797554350_20998753c0.jpg\n",
      ">1798209205_77dbf525b0.jpg\n",
      ">1798215547_ef7ad95be8.jpg\n",
      ">179829865_095b040377.jpg\n",
      ">1799188614_b5189728ba.jpg\n",
      ">1799271536_6e69c8f1dc.jpg\n",
      ">1800601130_1c0f248d12.jpg\n",
      ">180094434_b0f244832d.jpg\n",
      ">1801063894_60bce29e19.jpg\n",
      ">1801188148_a176954965.jpg\n",
      ">1801663973_5ad393caeb.jpg\n",
      ">1801874841_4c12055e2f.jpg\n",
      ">1802092493_7b44fdb6b9.jpg\n",
      ">1803631090_05e07cc159.jpg\n",
      ">180506881_de0f59770f.jpg\n",
      ">1805990081_da9cefe3a5.jpg\n",
      ">1806580620_a8fe0fb9f8.jpg\n",
      ">1807169176_7f5226bf5a.jpg\n",
      ">1808007704_ee8a93abb4.jpg\n",
      ">1808370027_2088394eb4.jpg\n",
      ">1808504612_3508f3c9bb.jpg\n",
      ">1809758121_96026913bb.jpg\n",
      ">1809796012_a2dac6c26b.jpg\n",
      ">1810651611_35aae644fb.jpg\n",
      ">181103691_fb2f956abd.jpg\n",
      ">181157221_e12410ef0b.jpg\n",
      ">1812525037_528465037c.jpg\n",
      ">1813266419_08bf66fe98.jpg\n",
      ">1813597483_3f09d2a020.jpg\n",
      ">1813777902_07d1d4b00c.jpg\n",
      ">1814086703_33390d5fc7.jpg\n",
      ">181415975_2627aa6668.jpg\n",
      ">1814391289_83a1eb71d3.jpg\n",
      ">181777261_84c48b31cb.jpg\n",
      ">1818403842_553a2a392c.jpg\n",
      ">1819261140_6c022f4b1d.jpg\n",
      ">1821238649_2fda79d6d7.jpg\n",
      ">182493240_40410254b0.jpg\n",
      ">1827560917_c8d3c5627f.jpg\n",
      ">1835511273_790eaae6e6.jpg\n",
      ">1836335410_de8313a64e.jpg\n",
      ">1837976956_3c45d0f9b8.jpg\n",
      ">185057637_e8ada37343.jpg\n",
      ">1858963639_4588cd4be9.jpg\n",
      ">185972340_781d60ccfd.jpg\n",
      ">1859726819_9a793b3b44.jpg\n",
      ">1859941832_7faf6e5fa9.jpg\n",
      ">1860543210_47e94cf652.jpg\n",
      ">186346360_541047336f.jpg\n",
      ">186348874_75b2cf1ec5.jpg\n",
      ">1865794069_6e3a1e57bb.jpg\n",
      ">186890601_8a6b0f1769.jpg\n",
      ">186890605_ddff5b694e.jpg\n",
      ">1874617189_e85d3f4326.jpg\n",
      ">1876536922_8fdf8d7028.jpg\n",
      ">1881494074_1bebd93089.jpg\n",
      ">1884065356_c6c34b4568.jpg\n",
      ">1884727806_d84f209868.jpg\n",
      ">1895768965_43cd9d164f.jpg\n",
      ">1897025969_0c41688fa6.jpg\n",
      ">189721896_1ffe76d89e.jpg\n",
      ">189740668_0b045f1ff2.jpg\n",
      ">1904112245_549e47c8aa.jpg\n",
      ">190638179_be9da86589.jpg\n",
      ">190965502_0b9ed331d9.jpg\n",
      ">191003283_992257f835.jpg\n",
      ">191003284_1025b0fb7d.jpg\n",
      ">191003285_edd8d0cf58.jpg\n",
      ">191003287_2915c11d8e.jpg\n",
      ">191592626_477ef5e026.jpg\n",
      ">1917203130_fcaff8b10e.jpg\n",
      ">1917265421_aeccf1ca38.jpg\n",
      ">1918573100_d31cbb6b77.jpg\n",
      ">19212715_20476497a3.jpg\n",
      ">1921398767_771743bf4e.jpg\n",
      ">1923476156_e20976b32d.jpg\n",
      ">1924234308_c9ddcf206d.jpg\n",
      ">1925434818_2949a8f6d8.jpg\n",
      ">1926129518_4350f4f552.jpg\n",
      ">1928319708_ccf1f4ee72.jpg\n",
      ">1931690777_897a7d8ab6.jpg\n",
      ">1932161768_996eadac87.jpg\n",
      ">1932314876_9cc46fd054.jpg\n",
      ">1936215201_d03a75cbba.jpg\n",
      ">1937104503_313d22a2d0.jpg\n",
      ">1937262236_cbf5bfa101.jpg\n",
      ">1947351225_288d788983.jpg\n",
      ">195084264_72fb347b0f.jpg\n",
      ">1952896009_cee8147c90.jpg\n",
      ">1956678973_223cb1b847.jpg\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">1956944011_c5661d3f22.jpg\n",
      ">1957371533_62bc720bac.jpg\n",
      ">195962284_e57178054a.jpg\n",
      ">195962790_3380aea352.jpg\n",
      ">1962729184_6996e128e7.jpg\n",
      ">1965278563_8279e408de.jpg\n",
      ">1969573381_5ecfae4c80.jpg\n",
      ">197107117_4b438b1872.jpg\n",
      ">197142902_f05ff198c2.jpg\n",
      ">197504190_fd1fc3d4b7.jpg\n",
      ">1975171469_84e425f61b.jpg\n",
      ">1975531316_8b00eeaaf7.jpg\n",
      ">1977827746_4e13d7e19f.jpg\n",
      ">197924859_f6e39a7dfa.jpg\n",
      ">1980315248_82dbc34676.jpg\n",
      ">1980882959_9a161f3469.jpg\n",
      ">1982852140_56425fa7a2.jpg\n",
      ">1984936420_3f3102132b.jpg\n",
      ">1989145280_3b54452188.jpg\n",
      ">1991806812_065f747689.jpg\n",
      ">1994416869_4dd769a806.jpg\n",
      ">199463720_329a802206.jpg\n",
      ">199809190_e3f6bbe2bc.jpg\n",
      ">1998255400_0cd086908f.jpg\n",
      ">1998457059_c9ac9a1e1a.jpg\n",
      ">1999444757_1b92efb590.jpg\n",
      ">2000459828_3c9e109106.jpg\n",
      ">2003663004_5b70920a98.jpg\n",
      ">2004674713_2883e63c67.jpg\n",
      ">200771289_31902164a7.jpg\n",
      ">2009636597_e3f4fe19fb.jpg\n",
      ">201682811_105241dee3.jpg\n",
      ">2017276266_566656c59d.jpg\n",
      ">2021602343_03023e1fd1.jpg\n",
      ">2021613437_d99731f986.jpg\n",
      ">2021671653_567395c7cf.jpg\n",
      ">2029280005_a19609c81a.jpg\n",
      ">2030781555_b7ff7be28f.jpg\n",
      ">203114209_e4cd71a6b7.jpg\n",
      ">2034553054_b00c166895.jpg\n",
      ">2036407732_d5a0389bba.jpg\n",
      ">2038662925_f4fa8c2534.jpg\n",
      ">2039457436_fc30f5e1ce.jpg\n",
      ">2040941056_7f5fd50794.jpg\n",
      ">2041867793_552819a40b.jpg\n",
      ">2042009399_afad34e7c1.jpg\n",
      ">2043427251_83b746da8e.jpg\n",
      ">2043520315_4a2c782c90.jpg\n",
      ">2044063458_fcc76a7636.jpg\n",
      ">2045023435_181854c013.jpg\n",
      ">2045109977_b00ec93491.jpg\n",
      ">2045562030_654ddea5e5.jpg\n",
      ">2045928594_92510c1c2a.jpg\n",
      ">2046222127_a6f300e202.jpg\n",
      ">2046778775_0dd7cac6ab.jpg\n",
      ">2049051050_20359a434a.jpg\n",
      ">2049646140_d0de01e3c4.jpg\n",
      ">2050067751_22d2763fd2.jpg\n",
      ">2051194177_fbeee211e3.jpg\n",
      ">2052202553_373dad145b.jpg\n",
      ">2052702658_da1204f6d1.jpg\n",
      ">2052953131_30834196fb.jpg\n",
      ">2053006423_6adf69ca67.jpg\n",
      ">2053441349_a98b5fc742.jpg\n",
      ">2053733930_e245615ad4.jpg\n",
      ">2053777548_108e54c826.jpg\n",
      ">2054308369_f9c6ec7815.jpg\n",
      ">2054869561_ff723e9eab.jpg\n",
      ">2055646179_169807fed4.jpg\n",
      ">2056041678_d6b5b39b26.jpg\n",
      ">2056042552_f59e338533.jpg\n",
      ">2056377805_e9a9b3bcf0.jpg\n",
      ">205682549_713aa6cd88.jpg\n",
      ">2056930414_d2b0f1395a.jpg\n",
      ">2057160636_6e9cf3b5f0.jpg\n",
      ">2057305043_952b8dc82c.jpg\n",
      ">2057306459_2f52ce648e.jpg\n",
      ">2058091220_2087270068.jpg\n",
      ">2058124718_89822bc96e.jpg\n",
      ">2058472558_7dd5014abd.jpg\n",
      ">2059616165_b7c99c1009.jpg\n",
      ">2059842472_f4fb61ea08.jpg\n",
      ">2060031241_a3ae7a06bb.jpg\n",
      ">206087108_d4557d38ee.jpg\n",
      ">2061144717_5b3a1864f0.jpg\n",
      ">2061354254_faa5bd294b.jpg\n",
      ">2061397486_53a61e17c5.jpg\n",
      ">2061927950_dafba5b8a3.jpg\n",
      ">2061944672_0383e65c8a.jpg\n",
      ">2062607137_dac194ad02.jpg\n",
      ">2063277300_f7ff476914.jpg\n",
      ">2064417101_3b9d817f4a.jpg\n",
      ">2064780645_8f28a1529f.jpg\n",
      ">2064790732_219e52e19c.jpg\n",
      ">2064792226_97e41d8167.jpg\n",
      ">2065309381_705b774f51.jpg\n",
      ">2065875490_a46b58c12b.jpg\n",
      ">2066048248_f53f5ef5e2.jpg\n",
      ">2066241589_b80e9f676c.jpg\n",
      ">2066271441_1f1f056c01.jpg\n",
      ">2067362863_59577f9d4d.jpg\n",
      ">2067833088_04e84e5bf2.jpg\n",
      ">2068403258_2669cf9763.jpg\n",
      ">2068465241_3bcabacfd7.jpg\n",
      ">2068960566_21e85ae0dc.jpg\n",
      ">2069279767_fb32bfb2de.jpg\n",
      ">2070798293_6b9405e04d.jpg\n",
      ">2070831281_dc04b3e15d.jpg\n",
      ">2070831523_5035d5537e.jpg\n",
      ">2071309418_1d7580b0f0.jpg\n",
      ">207237775_fa0a15c6fe.jpg\n",
      ">2072574835_febf0c5fb9.jpg\n",
      ">207275121_ee4dfa0bf2.jpg\n",
      ">2073105823_6dacade004.jpg\n",
      ">2073174497_18b779999c.jpg\n",
      ">2073756099_7e02c0110c.jpg\n",
      ">2073964624_52da3a0fc4.jpg\n",
      ">2074146683_7c83167aa1.jpg\n",
      ">2074244690_82e30ff44b.jpg\n",
      ">2074764331_90a9962b52.jpg\n",
      ">2075041394_0b3ea1822d.jpg\n",
      ">2075321027_c8fcbaf581.jpg\n",
      ">2075493556_b763648389.jpg\n",
      ">207584893_63e73c5c28.jpg\n",
      ">2076428547_738e0a132f.jpg\n",
      ">2076865206_53918c820c.jpg\n",
      ">2076906555_c20dc082db.jpg\n",
      ">2077079696_03380d218b.jpg\n",
      ">207731022_988f6afb35.jpg\n",
      ">2077346067_0a3a5aae65.jpg\n",
      ">2078311270_f01c9eaf4c.jpg\n",
      ">2079110798_ad1370a646.jpg\n",
      ">2079152458_40712c3b40.jpg\n",
      ">207930963_af3a2f1784.jpg\n",
      ">2079554580_f18d5c181b.jpg\n",
      ">2080033499_6be742f483.jpg\n",
      ">2081141788_38fa84ce3c.jpg\n",
      ">2081446176_f97dc76951.jpg\n",
      ">2081615901_13092cac56.jpg\n",
      ">2081679622_6f1442367d.jpg\n",
      ">2082005167_a0d6a70020.jpg\n",
      ">2083434441_a93bc6306b.jpg\n",
      ">2083778090_3aecaa11cc.jpg\n",
      ">2084103826_ffd76b1e3e.jpg\n",
      ">2084157130_f288e492e4.jpg\n",
      ">2084217208_7bd9bc85e5.jpg\n",
      ">2085078076_b9db242d21.jpg\n",
      ">2085255128_61224cc47f.jpg\n",
      ">2085400856_ae09df33a7.jpg\n",
      ">2085403342_a17b0654fe.jpg\n",
      ">2085557551_7a88d01d4e.jpg\n",
      ">2085726719_a57a75dbe5.jpg\n",
      ">2086513494_dbbcb583e7.jpg\n",
      ">2086532897_b8714f2237.jpg\n",
      ">2086534745_1e4ab80078.jpg\n",
      ">2086678529_b3301c2d71.jpg\n",
      ">2087317114_cf06df5aa5.jpg\n",
      ">2087640654_1a84577a44.jpg\n",
      ">2088120475_d6318364f5.jpg\n",
      ">2088460083_42ee8a595a.jpg\n",
      ">2088532947_c628e44c4a.jpg\n",
      ">2088910854_c6f8d4f5f9.jpg\n",
      ">2089122314_40d5739aef.jpg\n",
      ">2089350172_dc2cf9fcf6.jpg\n",
      ">2089426086_7acc98a3a8.jpg\n",
      ">2089442007_6fc798548c.jpg\n",
      ">2089539651_9e518ec7de.jpg\n",
      ">2089542487_b4c1ee7025.jpg\n",
      ">2089555297_95cf001fa7.jpg\n",
      ">2090327868_9f99e2740d.jpg\n",
      ">2090339522_d30d2436f9.jpg\n",
      ">2090386465_b6ebb7df2c.jpg\n",
      ">2090387793_274ab4cf7d.jpg\n",
      ">2090545563_a4e66ec76b.jpg\n",
      ">2090723611_318031cfa5.jpg\n",
      ">2090997177_76d482b158.jpg\n",
      ">2091171488_c8512fec76.jpg\n",
      ">2092177624_13ab757e8b.jpg\n",
      ">2092419948_eea8001d0f.jpg\n",
      ">2092870249_90e3f1855b.jpg\n",
      ">2094323311_27d58b1513.jpg\n",
      ">2094543127_46d2f1fedf.jpg\n",
      ">2094810449_f8df9dcdf7.jpg\n",
      ">2095007523_591f255708.jpg\n",
      ">2095078658_c14ba89bc2.jpg\n",
      ">2095435987_1b7591d214.jpg\n",
      ">2095444126_201ff9f222.jpg\n",
      ">2095478050_736c4d2d28.jpg\n",
      ">209605542_ca9cc52e7b.jpg\n",
      ">2096771662_984441d20d.jpg\n",
      ">2097398349_ff178b3f1b.jpg\n",
      ">2097403787_77a154f5b9.jpg\n",
      ">2097407245_c798e0dcaf.jpg\n",
      ">2097420505_439f63c863.jpg\n",
      ">2097489021_ca1b9f5c3b.jpg\n",
      ">2098174172_e57d86ea03.jpg\n",
      ">2098418613_85a0c9afea.jpg\n",
      ">2098646162_e3b3bbf14c.jpg\n",
      ">2099323664_bb20457f26.jpg\n",
      ">2100046085_69b59b6645.jpg\n",
      ">2100735137_05c6079537.jpg\n",
      ">2100816230_ff866fb352.jpg\n",
      ">2100909581_b7dde5b704.jpg\n",
      ">2101128963_fdf8b2a0d7.jpg\n",
      ">210126070_0d43b300b9.jpg\n",
      ">2101457132_69c950bc45.jpg\n",
      ">2101808682_0d66ef4a08.jpg\n",
      ">2102030040_2e8f4738f7.jpg\n",
      ">2102315758_a9148a842f.jpg\n",
      ">2102360862_264452db8e.jpg\n",
      ">2102581664_5ea50f85c6.jpg\n",
      ">2102724238_3cf921d7bb.jpg\n",
      ">2102732029_9ae520914d.jpg\n",
      ">2103361407_4ed4fc46bf.jpg\n",
      ">2103568100_5d018c495b.jpg\n",
      ">2105756457_a100d8434e.jpg\n",
      ">210625425_fb1ef5d23b.jpg\n",
      ">2106772874_381824648b.jpg\n",
      ">210686241_b8e069fff3.jpg\n",
      ">2107837987_ffecfc367a.jpg\n",
      ">2107838729_a527e434bd.jpg\n",
      ">210839948_bbd5bfa3b6.jpg\n",
      ">2108799322_e25aa6e185.jpg\n",
      ">2109370875_05241bdda7.jpg\n",
      ">2109479807_eec8d72ca7.jpg\n",
      ">2109911919_af45b93ef3.jpg\n",
      ">2110692070_8aaaa1ae39.jpg\n",
      ">2110898123_07729c1461.jpg\n",
      ">2111360187_d2505437b7.jpg\n",
      ">2112661738_de71b60b88.jpg\n",
      ">211277478_7d43aaee09.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DEEPBL~1\\AppData\\Local\\Temp/ipykernel_5456/4271291570.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# extract features from all images\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Flickr8k_Dataset'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Extracted Features: %d'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;31m# save to file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\DEEPBL~1\\AppData\\Local\\Temp/ipykernel_5456/4271291570.py\u001b[0m in \u001b[0;36mextract_features\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# get features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mfeature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[1;31m# get image id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mimage_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1725\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1726\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1727\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1728\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1729\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    922\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 924\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    925\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from pickle import dump\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model\n",
    " \n",
    "# extract features from each photo in the directory\n",
    "def extract_features(directory):\n",
    "    # load the model\n",
    "    model = VGG16()\n",
    "    # re-structure the model\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    # summarize\n",
    "    print(model.summary())\n",
    "    # extract features from each photo\n",
    "    features = dict()\n",
    "    for name in listdir(directory):\n",
    "        # load an image from file\n",
    "        filename = directory + '/' + name\n",
    "        image = load_img(filename, target_size=(224, 224))\n",
    "        # convert the image pixels to a numpy array\n",
    "        image = img_to_array(image)\n",
    "        # reshape data for the model\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        # prepare the image for the VGG model\n",
    "        image = preprocess_input(image)\n",
    "        # get features\n",
    "        feature = model.predict(image, verbose=0)\n",
    "        # get image id\n",
    "        image_id = name.split('.')[0]\n",
    "        # store feature\n",
    "        features[image_id] = feature\n",
    "        print('>%s' % name)\n",
    "    return features\n",
    "\n",
    "# extract features from all images\n",
    "directory = 'Flickr8k_Dataset'\n",
    "features = extract_features(directory)\n",
    "print('Extracted Features: %d' % len(features))\n",
    "# save to file\n",
    "dump(features, open('features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d4283",
   "metadata": {},
   "source": [
    "# Prepare text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "617121d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 8092 \n",
      "Vocabulary Size: 8763\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# extract descriptions for images\n",
    "def load_descriptions(doc):\n",
    "    mapping = dict()\n",
    "    # process lines\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        # take the first token as the image id, the rest as the description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # remove filename from image id\n",
    "        image_id = image_id.split('.')[0]\n",
    "        # convert description tokens back to string\n",
    "        image_desc = ' '.join(image_desc)\n",
    "        # create the list if needed\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = list()\n",
    "        # store description\n",
    "        mapping[image_id].append(image_desc)\n",
    "    return mapping\n",
    "\n",
    "def clean_descriptions(descriptions):\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for i in range(len(desc_list)):\n",
    "            desc = desc_list[i]\n",
    "            # tokenize\n",
    "            desc = desc.split()\n",
    "            # convert to lower case\n",
    "            desc = [word.lower() for word in desc]\n",
    "            # remove punctuation from each token\n",
    "            desc = [w.translate(table) for w in desc]\n",
    "            # remove hanging 's' and 'a'\n",
    "            desc = [word for word in desc if len(word)>1]\n",
    "            # remove tokens with numbers in them\n",
    "            desc = [word for word in desc if word.isalpha()]\n",
    "            # store as string\n",
    "            desc_list[i] =  ' '.join(desc)\n",
    "\n",
    "# convert the loaded descriptions into a vocabulary of words\n",
    "def to_vocabulary(descriptions):\n",
    "    # build a list of all description strings\n",
    "    all_desc = set()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "# save descriptions to file, one per line\n",
    "def save_descriptions(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + ' ' + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "filename = 'Flickr8k_text/Flickr8k.token.txt'\n",
    "# load descriptions\n",
    "doc = load_doc(filename)\n",
    "# parse descriptions\n",
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: %d ' % len(descriptions))\n",
    "# clean descriptions\n",
    "clean_descriptions(descriptions)\n",
    "# summarize vocabulary\n",
    "vocabulary = to_vocabulary(descriptions)\n",
    "print('Vocabulary Size: %d' % len(vocabulary))\n",
    "# save to file\n",
    "save_descriptions(descriptions, 'descriptions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f31c48",
   "metadata": {},
   "source": [
    "# Develop Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17ac6101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n",
      "Photos: train=6000\n",
      "Vocabulary Size: 7579\n",
      "Description Length: 34\n",
      "Dataset: 1000\n",
      "Descriptions: test=1000\n",
      "Photos: test=1000\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 34)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 34, 256)      1940224     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 4096)         0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 34, 256)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          1048832     dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 256)          525312      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 256)          0           dense[0][0]                      \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 7579)         1947803     dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,527,963\n",
      "Trainable params: 5,527,963\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DEEPBL~1\\AppData\\Local\\Temp/ipykernel_4040/3169073647.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[0mcheckpoint\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mModelCheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmonitor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_best_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'min'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;31m# fit model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX1train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX2train\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX1test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX2test\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mytest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    # process line by line\n",
    "    for line in doc.split('\\n'):\n",
    "        # skip empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # get the image identifier\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)\n",
    "\n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    # load document\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        # split id from description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # skip images not in the set\n",
    "        if image_id in dataset:\n",
    "            # create list\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            # wrap description in tokens\n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            # store\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions\n",
    "\n",
    "# load photo features\n",
    "def load_photo_features(filename, dataset):\n",
    "    # load all features\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    # filter features\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features\n",
    "\n",
    "# covert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# calculate the length of the description with the most words\n",
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)\n",
    "\n",
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, descriptions, photos, vocab_size):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    # walk through each image identifier\n",
    "    for key, desc_list in descriptions.items():\n",
    "        # walk through each description for the image\n",
    "        for desc in desc_list:\n",
    "            # encode the sequence\n",
    "            seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "            # split one sequence into multiple X,y pairs\n",
    "            for i in range(1, len(seq)):\n",
    "                # split into input and output pair\n",
    "                in_seq, out_seq = seq[:i], seq[i]\n",
    "                # pad input sequence\n",
    "                in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                # encode output sequence\n",
    "                out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                # store\n",
    "                X1.append(photos[key][0])\n",
    "                X2.append(in_seq)\n",
    "                y.append(out_seq)\n",
    "    return array(X1), array(X2), array(y)\n",
    "\n",
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "    # feature extractor model\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    # sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    # decoder model\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    # summarize model\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "# train dataset\n",
    "\n",
    "# load training dataset (6K)\n",
    "filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features('features.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)\n",
    "# prepare sequences\n",
    "X1train, X2train, ytrain = create_sequences(tokenizer, max_length, train_descriptions, train_features, vocab_size)\n",
    "\n",
    "# dev dataset\n",
    "\n",
    "# load test set\n",
    "filename = 'Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "test = load_set(filename)\n",
    "print('Dataset: %d' % len(test))\n",
    "# descriptions\n",
    "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "# photo features\n",
    "test_features = load_photo_features('features.pkl', test)\n",
    "print('Photos: test=%d' % len(test_features))\n",
    "# prepare sequences\n",
    "X1test, X2test, ytest = create_sequences(tokenizer, max_length, test_descriptions, test_features, vocab_size)\n",
    "\n",
    "# fit model\n",
    "\n",
    "# define the model\n",
    "model = define_model(vocab_size, max_length)\n",
    "# define checkpoint callback\n",
    "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "# fit model\n",
    "model.fit([X1train, X2train], ytrain, epochs=20, verbose=2, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d9126a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n",
      "Photos: train=6000\n",
      "Vocabulary Size: 7579\n",
      "Description Length: 34\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 34)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 4096)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 34, 256)      1940224     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 4096)         0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 34, 256)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 256)          1048832     dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          525312      dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 256)          0           dense_3[0][0]                    \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          65792       add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 7579)         1947803     dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,527,963\n",
      "Trainable params: 5,527,963\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "('You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) ', 'for plot_model/model_to_dot to work.')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Deep Blade\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 2536s 422ms/step - loss: 4.7117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Deep Blade\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000/6000 [==============================] - 2542s 424ms/step - loss: 3.9176\n",
      "6000/6000 [==============================] - 2497s 416ms/step - loss: 3.6606\n",
      "6000/6000 [==============================] - 2485s 414ms/step - loss: 3.5052\n",
      "6000/6000 [==============================] - 3286s 548ms/step - loss: 3.4153\n",
      "  51/6000 [..............................] - ETA: 45:10 - loss: 3.2346"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DEEPBL~1\\AppData\\Local\\Temp/ipykernel_4040/1208168748.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[0mgenerator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_descriptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[1;31m# fit for one epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m     \u001b[1;31m# save model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model_'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1941\u001b[0m                   \u001b[1;34m'will be removed in a future version. '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1942\u001b[0m                   'Please use `Model.fit`, which supports generators.')\n\u001b[1;32m-> 1943\u001b[1;33m     return self.fit(\n\u001b[0m\u001b[0;32m   1944\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1945\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3023\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1960\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1961\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.merge import add\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    # process line by line\n",
    "    for line in doc.split('\\n'):\n",
    "        # skip empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # get the image identifier\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)\n",
    "\n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    # load document\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        # split id from description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # skip images not in the set\n",
    "        if image_id in dataset:\n",
    "            # create list\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            # wrap description in tokens\n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            # store\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions\n",
    "\n",
    "# load photo features\n",
    "def load_photo_features(filename, dataset):\n",
    "    # load all features\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    # filter features\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features\n",
    "\n",
    "# covert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# calculate the length of the description with the most words\n",
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)\n",
    "\n",
    "# create sequences of images, input sequences and output words for an image\n",
    "def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n",
    "    X1, X2, y = list(), list(), list()\n",
    "    # walk through each description for the image\n",
    "    for desc in desc_list:\n",
    "        # encode the sequence\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        # split one sequence into multiple X,y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            # split into input and output pair\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # pad input sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            # encode output sequence\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            # store\n",
    "            X1.append(photo)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    return array(X1), array(X2), array(y)\n",
    "\n",
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "    # feature extractor model\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    # sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    # decoder model\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    # compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    # summarize model\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "# data generator, intended to be used in a call to model.fit_generator()\n",
    "def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n",
    "    # loop for ever over images\n",
    "    while 1:\n",
    "        for key, desc_list in descriptions.items():\n",
    "            # retrieve the photo feature\n",
    "            photo = photos[key][0]\n",
    "            in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n",
    "            yield [in_img, in_seq], out_word\n",
    "\n",
    "# load training dataset (6K)\n",
    "filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# photo features\n",
    "train_features = load_photo_features('features.pkl', train)\n",
    "print('Photos: train=%d' % len(train_features))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)\n",
    "\n",
    "# define the model\n",
    "model = define_model(vocab_size, max_length)\n",
    "# train the model, run epochs manually and save after each epoch\n",
    "epochs = 10\n",
    "steps = len(train_descriptions)\n",
    "for i in range(epochs):\n",
    "    # create the data generator\n",
    "    generator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\n",
    "    # fit for one epoch\n",
    "    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
    "    # save model\n",
    "    model.save('model_' + str(i) + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dbd6a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n",
      "Vocabulary Size: 7579\n",
      "Description Length: 34\n",
      "Dataset: 1000\n",
      "Descriptions: test=1000\n",
      "Photos: test=1000\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "BLEU-1: 0.529691\n",
      "BLEU-2: 0.277107\n",
      "BLEU-3: 0.182869\n",
      "BLEU-4: 0.080318\n"
     ]
    }
   ],
   "source": [
    "from numpy import argmax\n",
    "from pickle import load\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    # process line by line\n",
    "    for line in doc.split('\\n'):\n",
    "        # skip empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # get the image identifier\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)\n",
    "\n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    # load document\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        # split id from description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # skip images not in the set\n",
    "        if image_id in dataset:\n",
    "            # create list\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            # wrap description in tokens\n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            # store\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions\n",
    "\n",
    "# load photo features\n",
    "def load_photo_features(filename, dataset):\n",
    "    # load all features\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    # filter features\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features\n",
    "\n",
    "# covert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# calculate the length of the description with the most words\n",
    "def max_length(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    return max(len(d.split()) for d in lines)\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(max_length):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text\n",
    "\n",
    "# evaluate the skill of the model\n",
    "def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n",
    "    actual, predicted = list(), list()\n",
    "    # step over the whole set\n",
    "    for key, desc_list in descriptions.items():\n",
    "        # generate description\n",
    "        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n",
    "        # store actual and predicted\n",
    "        references = [d.split() for d in desc_list]\n",
    "        actual.append(references)\n",
    "        predicted.append(yhat.split())\n",
    "    # calculate BLEU score\n",
    "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
    "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
    "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
    "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n",
    "\n",
    "# prepare tokenizer on train set\n",
    "\n",
    "# load training dataset (6K)\n",
    "filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary Size: %d' % vocab_size)\n",
    "# determine the maximum sequence length\n",
    "max_length = max_length(train_descriptions)\n",
    "print('Description Length: %d' % max_length)\n",
    "\n",
    "# prepare test set\n",
    "\n",
    "# load test set\n",
    "filename = 'Flickr8k_text/Flickr_8k.testImages.txt'\n",
    "test = load_set(filename)\n",
    "print('Dataset: %d' % len(test))\n",
    "# descriptions\n",
    "test_descriptions = load_clean_descriptions('descriptions.txt', test)\n",
    "print('Descriptions: test=%d' % len(test_descriptions))\n",
    "# photo features\n",
    "test_features = load_photo_features('features.pkl', test)\n",
    "print('Photos: test=%d' % len(test_features))\n",
    "\n",
    "# load the model\n",
    "filename = 'model_18.h5'\n",
    "model = load_model(filename)\n",
    "# evaluate model\n",
    "evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8db5f75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 6000\n",
      "Descriptions: train=6000\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from pickle import dump\n",
    "\n",
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    # process line by line\n",
    "    for line in doc.split('\\n'):\n",
    "        # skip empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # get the image identifier\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)\n",
    "\n",
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    # load document\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        # split id from description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # skip images not in the set\n",
    "        if image_id in dataset:\n",
    "            # create list\n",
    "            if image_id not in descriptions:\n",
    "                descriptions[image_id] = list()\n",
    "            # wrap description in tokens\n",
    "            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "            # store\n",
    "            descriptions[image_id].append(desc)\n",
    "    return descriptions\n",
    "\n",
    "# covert a dictionary of clean descriptions to a list of descriptions\n",
    "def to_lines(descriptions):\n",
    "    all_desc = list()\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "\n",
    "# fit a tokenizer given caption descriptions\n",
    "def create_tokenizer(descriptions):\n",
    "    lines = to_lines(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    "\n",
    "# load training dataset (6K)\n",
    "filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n",
    "train = load_set(filename)\n",
    "print('Dataset: %d' % len(train))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "print('Descriptions: train=%d' % len(train_descriptions))\n",
    "# prepare tokenizer\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b9bfe4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "startseq boy in blue shirt is jumping into the water endseq\n"
     ]
    }
   ],
   "source": [
    "from pickle import load\n",
    "from numpy import argmax\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "\n",
    "# extract features from each photo in the directory\n",
    "def extract_features(filename):\n",
    "    # load the model\n",
    "    model = VGG16()\n",
    "    # re-structure the model\n",
    "    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
    "    # load the photo\n",
    "    image = load_img(filename, target_size=(224, 224))\n",
    "    # convert the image pixels to a numpy array\n",
    "    image = img_to_array(image)\n",
    "    # reshape data for the model\n",
    "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "    # prepare the image for the VGG model\n",
    "    image = preprocess_input(image)\n",
    "    # get features\n",
    "    feature = model.predict(image, verbose=0)\n",
    "    return feature\n",
    "\n",
    "# map an integer to a word\n",
    "def word_for_id(integer, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == integer:\n",
    "            return word\n",
    "    return None\n",
    "\n",
    "# generate a description for an image\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # seed the generation process\n",
    "    in_text = 'startseq'\n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(max_length):\n",
    "        # integer encode input sequence\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # pad input\n",
    "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "        # predict next word\n",
    "        yhat = model.predict([photo,sequence], verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = word_for_id(yhat, tokenizer)\n",
    "        # stop if we cannot map the word\n",
    "        if word is None:\n",
    "            break\n",
    "        # append as input for generating the next word\n",
    "        in_text += ' ' + word\n",
    "        # stop if we predict the end of the sequence\n",
    "        if word == 'endseq':\n",
    "            break\n",
    "    return in_text\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))\n",
    "# pre-define the max sequence length (from training)\n",
    "max_length = 34\n",
    "# load the model\n",
    "model = load_model('model_18.h5')\n",
    "# load and prepare the photograph\n",
    "photo = extract_features('11.jpg')\n",
    "# generate description\n",
    "description = generate_desc(model, tokenizer, photo, max_length)\n",
    "print(description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78106b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man in red shirt is sitting on bench with his arms out to the camera\n"
     ]
    }
   ],
   "source": [
    "query = description\n",
    "stopwords = ['startseq', 'endseq']\n",
    "querywords = query.split()\n",
    "resultwords = [word for word in querywords if word.lower() not in stopwords]\n",
    "result = ' '.join(resultwords)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c416073f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "283f75e4a4fbcd0d6ee4fa8c9aaee10db35f33ad42a00c526d6e36fa6bc45727"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
